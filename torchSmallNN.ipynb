{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "023c639e-b08a-496a-9767-c5abd6e9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch \n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3a05255-6d39-46fd-9244-369796ce099a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer: \n",
    "    def __init__ (Id, Od, p_activ, final = False):\n",
    "        self.W = (torch.randn(Od, Id) * torch.sqrt(2/Id)).to(device)\n",
    "        self.dW = torch.zeros_like(self.W).to(device)\n",
    "        \n",
    "        self.B, self.dB, self.dB_rms, self.dB_vel, self.C, self.dC, self.dC_rms, \n",
    "        self.dC_vel, self.V_avg, self.C_avg, self.dV, self.dV_rms, self.dV_vel = [torch.zeros(Od, 1).to(device) for _ in range (0,13)]\n",
    "        self.p = p_active\n",
    "        \n",
    "        self.V = torch.ones_like(self.C).to(device)\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.dA_prev = None\n",
    "        self.final = final \n",
    "        \n",
    "        def opt(self, opt_params):\n",
    "            f.optimize(self, opt_params)\n",
    "        \n",
    "        def fp (self, A_prev, Y):\n",
    "            if self.final:\n",
    "                f.fin_fprop(self, A_prev, Y)\n",
    "            else:\n",
    "                f.fprop(self, A_prev, Y)\n",
    "        \n",
    "        def bp (self, dA, Y): \n",
    "            if self.final:\n",
    "                f.fin_bprop(self, dA, Y)\n",
    "            else:\n",
    "                f.bprop(self, dA, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4f9f562-afea-4ad5-9bc7-2818357e62ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class f: \n",
    "    @staticmethod\n",
    "    def prelu (Z,p): \n",
    "        return torch.where(Z>0, Z, p * Z)\n",
    "        \n",
    "    @staticmethod\n",
    "    def prelu_grad (Z,p): \n",
    "        return torch.where(Z>0, 1, p) \n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax (Z): \n",
    "        Z_exp = torch.exp(Z - torch.max(Z))\n",
    "        return Z_exp/torch.sum(Z_exp, dim = 0, keepdim = True)\n",
    "        \n",
    "    @staticmethod\n",
    "    def crossENT (A,Y): \n",
    "        m = A.shape[1]\n",
    "        L = -torch.sum(Y * torch.log(A), dim = 0, keepdim = True)\n",
    "        J = torch.sum(L)/m \n",
    "        return J\n",
    "    \n",
    "    @staticmethod\n",
    "    def fprop (layer, A_prev, Y):\n",
    "        e = 0.0000000000001\n",
    "        Z = (torch.matmul(layer.W, A_prev) + layer.B).to(device)\n",
    "        mean = torch.mean(Z, dim = 1, keepdim = True).to(device)\n",
    "        std = (torch.std(Z, dim = 1, keepdim = True) + e).to(device)\n",
    "        Z_ = ((Z - mean)/std).to(device)\n",
    "        H = ((layer.V * Z_) + layer.C).to(device)\n",
    "        A = (f.prelu(H, layer.p)).to(device)\n",
    "        layer.cache = {'A_prev': A_prev.to(device),\n",
    "                       'Z' : Z,\n",
    "                       'std': std,\n",
    "                       'H': H,\n",
    "                       'Z_': Z_,\n",
    "                       'A' : A\n",
    "                      }\n",
    "        \n",
    "    @staticmethod\n",
    "    def fin_fprop (layer, A_prev, Y):\n",
    "        e = 0.0000000000001\n",
    "        Z = (torch.matmul(layer.W, A_prev) + layer.B).to(device)\n",
    "        mean = torch.mean(Z, dim = 1, keepdim = True).to(device)\n",
    "        std = (torch.std(Z, dim = 1, keepdim = True) + e).to(device)\n",
    "        Z_ = ((Z - mean)/std).to(device)\n",
    "        H = ((layer.V * Z_) + layer.C).to(device)\n",
    "        A = (f.softmax(H, layer.p)).to(device)\n",
    "        layer.cache = {'A_prev': A_prev.to(device),\n",
    "                       'Z' : Z,\n",
    "                       'std': std,\n",
    "                       'H': H,\n",
    "                       'Z_': Z_,\n",
    "                       'A' : A\n",
    "                      }\n",
    "        self.cost = f.crossENT(A, Y)\n",
    "        \n",
    "    @staticmethod\n",
    "    def bprop (layers, dA, Y):\n",
    "            A_prev = layer.cache['A_prev'].to(device)\n",
    "            Z = layer.cache['Z'].to(device)\n",
    "            Z_ = layer.cache['Z_'].to(device)\n",
    "            A = layer.cache['A'].to(device)\n",
    "            m = A_prev.shape[1]\n",
    "            std = layer.cahse['std'].to(device)\n",
    "            dH = (dA * f.prelu_grad(H, layer.p)).to(device)\n",
    "            layer.dV = torch.sum((Z_ * dH), dim = 1, keepdim = True)\n",
    "            layer.dC = torch.sum(dH, dim = 1, keepdim = True)\n",
    "            dZ_ = (layer.V * dH).to(device)\n",
    "            dZ = (((m*dZ_) - (torch.sum(dZ_, dim = 1, keepdim = True)) - (Z_ * (toch.sum((Z_ * dZ_), dim = 1, keepdims = True))))/(m * std)).to(device)\n",
    "            layer.dW = torch.matmul(dZ, A_prev.permute(1, 0))\n",
    "            layer.dB = torch.sum(dZ, dim = 1, keepdim = True)\n",
    "            layer.dA_prev = torch.matmul(W.permute(1, 0), dZ)\n",
    "                             \n",
    "    @staticmethod\n",
    "    def bprop (layers, dA, Y):\n",
    "            A_prev = layer.cache['A_prev'].to(device)\n",
    "            Z = layer.cache['Z'].to(device)\n",
    "            Z_ = layer.cache['Z_'].to(device)\n",
    "            A = layer.cache['A'].to(device)\n",
    "            m = A_prev.shape[1]\n",
    "            std = layer.cahse['std'].to(device)\n",
    "            dH = ((A - Y)/m).to(device)\n",
    "            layer.dV = torch.sum((Z_ * dH), dim = 1, keepdim = True)\n",
    "            layer.dC = torch.sum(dH, dim = 1, keepdim = True)\n",
    "            dZ_ = (layer.V * dH).to(device)\n",
    "            dZ = (((m*dZ_) - (torch.sum(dZ_, dim = 1, keepdim = True)) - (Z_ * (toch.sum((Z_ * dZ_), dim = 1, keepdims = True))))/(m * std)).to(device)\n",
    "            layer.dW = torch.matmul(dZ, A_prev.permute(1, 0))\n",
    "            layer.dB = torch.sum(dZ, dim = 1, keepdim = True)\n",
    "            layer.dA_prev = torch.matmul(W.permute(1, 0), dZ)\n",
    "    \n",
    "    @staticmethod\n",
    "    def lr_decay(t, alpha_0):\n",
    "        alph = alpha_0\n",
    "        return alph\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimize(layers, opt_params):\n",
    "        alpha_0 = opt_params['alpha_0']\n",
    "        t = opt_params['t']\n",
    "        alpha = f.lr_decay(t, alpha_0)\n",
    "        beta_dW, beta_dW0 = opt_params['beta_dW'] \n",
    "        beta_dB, beta_dB0 = opt_params['beta_dB']\n",
    "        beta_dC, beta_dC0 = opt_params['beta_dC']\n",
    "        theta_C = opt_params['theta_C']\n",
    "        theta_V = opt_params['theta_V']\n",
    "        \n",
    "        layer.dB_vel = (((beta_dB) * (layer.dB_vel)) + ((1-Beta_dB) * (layer.dB)))/(1 - (beta_dB ** t))\n",
    "        layer.dB_rms = (((beta_dB0) * (layer.dB_vel)) + ((1-Beta_dB0) * ((layer.dB) ** 2)))/(1 - (beta_dB0 ** t))\n",
    "                                                        \n",
    "        layer.dW_vel = (((beta_dW) * (layer.dW_vel)) + ((1-Beta_dW) * (layer.dW)))/(1 - (beta_dW ** t))\n",
    "        layer.dW_rms = (((beta_dW0) * (layer.dW_vel)) + ((1-Beta_dW0) * ((layer.dW) ** 2)))/(1 - (beta_dW0 ** t))\n",
    "                                                        \n",
    "        layer.dV_vel = (((beta_dV) * (layer.dV_vel)) + ((1-Beta_dV) * (layer.dV)))/(1 - (beta_dV ** t))\n",
    "        layer.dV_rms = (((beta_dV0) * (layer.dV_vel)) + ((1-Beta_dV0) * ((layer.dV) ** 2)))/(1 - (beta_dV0 ** t))\n",
    "                                                        \n",
    "        layer.dC_vel = (((beta_dC) * (layer.dC_vel)) + ((1-Beta_dC) * (layer.dC)))/(1 - (beta_dC ** t))\n",
    "        layer.dC_rms = (((beta_dC0) * (layer.dC_vel)) + ((1-Beta_dC0) * ((layer.dC) ** 2)))/(1 - (beta_dC0 ** t))\n",
    "                                                        \n",
    "        layer.V_avg = (((theta_V) * (layer.V_avg)) + ((1 - theta_V) * (layer.V)))/(1 - (theta_V ** t))\n",
    "        layer.C_avg = (((theta_C) * (layer.C_avg)) + ((1 - theta_C) * (layer.C)))/(1 - (theta_C ** t))\n",
    "        \n",
    "        layer.W -= (alpha) * (layer.dW_vel/torch.sqrt(layer.dW_rms))\n",
    "        layer.B-= (alpha) * (layer.dB_vel/torch.sqrt(layer.dB_rms))\n",
    "        layer.V -= (alpha) * (layer.dV_vel/torch.sqrt(layer.dV_rms))\n",
    "        layer.C -= (alpha) * (layer.dC_vel/torch.sqrt(layer.dC_rms))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62916445-3637-4134-9f4f-3bac16c38825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model: \n",
    "    def __init__ (self, A0_dim, AL_dim, HL_dims, p_activ): \n",
    "        self.L_dims = [A0_dim] + HL_dims + [AL_dim]\n",
    "        self.n = len(self.L_dims)\n",
    "        self.layers = [0]\n",
    "        self.layers += [Layer(self.L_dims[i-1], self.L_dims[i], p_activ, final = False) for i in range (1, self.n - 1)]\n",
    "        self.layers += [Layer(self.L_dims[self.n - 2], self.L_dims[self.n - 1], p_activ, final = True)]\n",
    "\n",
    "    def forward (self, X, Y):\n",
    "        A_prev = X\n",
    "        for i in range(1, self.n): \n",
    "            \n",
    "            self.layers[i].fp(A_prev, Y)\n",
    "            A_prev = self.layers[i].cache['A']\n",
    "            #self.fp = F.fprop(self, A_prev, Y) \n",
    "    def backward (self, X, Y): \n",
    "        dA = None \n",
    "        for i in reversed(range(1, self.n)): \n",
    "            self.layers[i].bp(dA, Y)\n",
    "            dA = self.layers[i].dA_prev\n",
    "\n",
    "    def gradient_descent(self, alph): \n",
    "        for i in range(1, self.n): \n",
    "            self.layers[i].opt(opt_params)\n",
    "\n",
    "    def training_epoch (self, opt)params, X_train_batches, Y_train_batches, printCost = False):\n",
    "        bs = len(X_train_batches)\n",
    "        for t in range (0,bs): \n",
    "            X_train = X_train_batches[t]\n",
    "            Y_train = Y_train_batches[t]\n",
    "            \n",
    "            self.forward(X_train, Y_train)\n",
    "            self.backward(X_train, Y_train)\n",
    "            self.gradient_descent(alph)\n",
    "            if printCost:\n",
    "                print(self.layers[self.n-1].cost)\n",
    "        \n",
    "    def predict (self, X, Y): \n",
    "        self.forward(X, Y)\n",
    "        fin_act = self.layers[self.n - 1].cache['A']\n",
    "        pred = torch.zeros_like(fin_act)\n",
    "        max_indices = torch.argmax(fin_act, dim=0)\n",
    "        pred.scatter_(0, max_indices.unsqueeze(0), 1)\n",
    "        return pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
