{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2cae8c5-e348-4c46-ae44-045c486a9ecc",
   "metadata": {
    "id": "d2cae8c5-e348-4c46-ae44-045c486a9ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# #we will import a bunch of stuff, and become a senior ML engineer\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ca1a5a-a62b-46c9-9b75-3a95bcac603d",
   "metadata": {
    "id": "88ca1a5a-a62b-46c9-9b75-3a95bcac603d"
   },
   "outputs": [],
   "source": [
    "class Activ:\n",
    "\n",
    "    def __init__(self,p):\n",
    "\n",
    "        def relu (x):\n",
    "            if isinstance(x, np.ndarray):\n",
    "                return np.where(x > 0, x, 0.0)\n",
    "            else:\n",
    "                if x > 0:\n",
    "                    return x\n",
    "                else:\n",
    "                    return 0.0\n",
    "\n",
    "        def relu_grad (x):\n",
    "            if isinstance(x, np.ndarray):\n",
    "                return np.where(x > 0, 1.0, 0.0)\n",
    "            else:\n",
    "                if x > 0:\n",
    "                    return 1.0\n",
    "                else:\n",
    "                    return 0.0\n",
    "\n",
    "        relu_dict = {}\n",
    "        relu_dict[\"activ\"] = relu\n",
    "        relu_dict[\"grad\"] = relu_grad\n",
    "\n",
    "        def binStep (x):\n",
    "           if isinstance(x, np.ndarray):\n",
    "               return np.where(x > 0, 1.0, 0.0)\n",
    "           else:\n",
    "                if x > 0:\n",
    "                    return 1.0\n",
    "                else:\n",
    "                    return 0.0\n",
    "\n",
    "        def binStep_grad (x):\n",
    "            return binStep(x)\n",
    "\n",
    "        binStep_dict = {}\n",
    "        binStep_dict[\"activ\"] = binStep\n",
    "        binStep_dict[\"grad\"] = binStep_grad\n",
    "\n",
    "        def sigmoid (x):\n",
    "            return (1/(1+np.exp(-x)))\n",
    "\n",
    "        def sigmoid_grad (x):\n",
    "            return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "        sigmoid_dict = {}\n",
    "        sigmoid_dict[\"activ\"] = sigmoid\n",
    "        sigmoid_dict[\"grad\"] = sigmoid_grad\n",
    "\n",
    "\n",
    "        def tanh (x):\n",
    "            return ((2/(1 + np.exp(-2 * x))) - 1)\n",
    "        def tanh_grad (x):\n",
    "            return (1 - (tanh(x)**2))\n",
    "\n",
    "        tanh_dict = {}\n",
    "        tanh_dict[\"activ\"] = tanh\n",
    "        tanh_dict[\"grad\"] = tanh_grad\n",
    "\n",
    "        def arctan (x):\n",
    "                return np.arctan(x)\n",
    "        def arctan_grad (x):\n",
    "                return (1/(1+(x**2)))\n",
    "\n",
    "        arctan_dict = {}\n",
    "        arctan_dict[\"activ\"] = arctan\n",
    "        arctan_dict[\"grad\"] = arctan_grad\n",
    "\n",
    "        def Parctan (x): \n",
    "            q = (2 * p)/(np.pi)\n",
    "            return q * ( np.arctan(x) + (np.pi/2))\n",
    "        def Parctan_grad (x):\n",
    "            q = (2 * p)/(np.pi)\n",
    "            return (q * (1/(1+(x**2))))\n",
    "            \n",
    "        Parctan_dict = {}\n",
    "        Parctan_dict[\"activ\"] = Parctan\n",
    "        Parctan_dict[\"grad\"] = Parctan_grad\n",
    "\n",
    "        def prelu (x):\n",
    "            if isinstance(x, np.ndarray):\n",
    "                return np.where(x > 0, x, p * x)\n",
    "            else:\n",
    "                if x > 0:\n",
    "                    return x\n",
    "                else:\n",
    "                    return p * x\n",
    "\n",
    "        def prelu_grad (x):\n",
    "            if isinstance(x, np.ndarray):\n",
    "                return np.where(x > 0, 1.0, p)\n",
    "            else:\n",
    "                if x > 0:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return p\n",
    "\n",
    "        prelu_dict = {}\n",
    "        prelu_dict[\"activ\"] = prelu\n",
    "        prelu_dict[\"grad\"] = prelu_grad\n",
    "\n",
    "        def elu (x):\n",
    "            if isinstance(x, np.ndarray):\n",
    "                return np.where(x > 0, x, (p * (np.exp(x) - 1)))\n",
    "            else:\n",
    "                if x > 0:\n",
    "                    return x\n",
    "                else:\n",
    "                    return (p * (np.exp(x) - 1))\n",
    "\n",
    "        def elu_grad (x):\n",
    "            if isinstance(x, np.ndarray):\n",
    "                return np.where(x > 0, 1.0, elu(x) + p)\n",
    "            else:\n",
    "                if x > 0:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return (elu(x) + p)\n",
    "\n",
    "        elu_dict = {}\n",
    "        elu_dict[\"activ\"] = elu\n",
    "        elu_dict[\"grad\"] = elu_grad\n",
    "\n",
    "        def softplus (x):\n",
    "            return (np.log(1 + np.exp(x)))\n",
    "\n",
    "        def softplus_grad (x):\n",
    "            return sigmoid(x)\n",
    "\n",
    "        softplus_dict = {}\n",
    "        softplus_dict[\"activ\"] = softplus\n",
    "        softplus_dict[\"grad\"] = softplus_grad\n",
    "\n",
    "        \n",
    "        Activators = {}\n",
    "        Activators[\"relu\"] = relu_dict\n",
    "        Activators[\"binStep\"] = binStep_dict\n",
    "        Activators[\"sigmoid\"] = sigmoid_dict\n",
    "        Activators[\"tanh\"] = tanh_dict\n",
    "        Activators[\"arctan\"] = arctan_dict\n",
    "        Activators[\"prelu\"] = prelu_dict\n",
    "        Activators[\"elu\"] = elu_dict\n",
    "        Activators[\"softplus\"] = softplus_dict\n",
    "        Activators[\"Parctan\"] = Parctan_dict\n",
    "        self.Activ_choices = Activators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6dae82e-4956-4472-81bf-9be5affab039",
   "metadata": {
    "id": "d6dae82e-4956-4472-81bf-9be5affab039"
   },
   "outputs": [],
   "source": [
    "class LossFuncs:\n",
    "    def __init__ (self,p):\n",
    "\n",
    "        def L2 (y_o, y):\n",
    "            return ((y_o - y)**2)\n",
    "        def L2_grad(y_o, y):\n",
    "            return (2 * (y_o - y))\n",
    "\n",
    "        L2_dict = {}\n",
    "        L2_dict[\"activ\"] = L2\n",
    "        L2_dict[\"grad\"] = L2_grad\n",
    "\n",
    "        self.e = 0.0001\n",
    "        def crossENT (y_o, y):\n",
    "            return -((y * np.log(y_o)) + ((1-y) * (np.log(1-y_o))))\n",
    "        def crossENT_grad (y_o, y):\n",
    "            return -(np.divide(y, y_o + self.e) - np.divide(1-y, (1-y_o) + self.e))\n",
    "\n",
    "        crossENT_dict = {}\n",
    "        crossENT_dict[\"activ\"] = crossENT\n",
    "        crossENT_dict[\"grad\"] = crossENT_grad\n",
    "\n",
    "\n",
    "        def L1 (y_o, y):\n",
    "            return np.abs(y_o - y)\n",
    "        def L1_grad (y_o, y):\n",
    "            return np.where(y_o - y > 0, 1, -1)\n",
    "\n",
    "        L1_dict = {}\n",
    "        L1_dict[\"activ\"] = L1\n",
    "        L1_dict[\"grad\"] = L1_grad\n",
    "\n",
    "\n",
    "        def Huber (y_o, y):\n",
    "            z = np.where(L1 (y_o,y) > p, (p * L1(y_o,y)) - (0.5 * (p ** 2)), L1 (y_o,y))\n",
    "            return (z * z)/2\n",
    "        def Huber_grad (y_o, y):\n",
    "            z = np.where(L1 (y_o,y) > p, (p * L1 (y_o,y)) - (0.5 * (p ** 2)), L1 (y_o,y))\n",
    "            dz = np.where(L1 (y_o,y) > p, p * L1_grad (y_o,y), L1_grad (y_o,y))\n",
    "            return z * dz\n",
    "\n",
    "        Huber_dict = {}\n",
    "        Huber_dict[\"activ\"] = Huber\n",
    "        Huber_dict[\"grad\"] = Huber_grad\n",
    "\n",
    "        def hinge (y_o, y):\n",
    "            return np.maximum(0, (1 - (y * y_o)))\n",
    "        def hinge_grad (y_o, y):\n",
    "            return np.where((1-(y * y_o)) > 0, -1, 0.0)\n",
    "\n",
    "        hinge_dict = {}\n",
    "        hinge_dict[\"activ\"] = hinge\n",
    "        hinge_dict[\"grad\"] = hinge_grad\n",
    "\n",
    "        Losses = {}\n",
    "        Losses[\"L2\"] = L2_dict\n",
    "        Losses[\"L1\"] = L1_dict\n",
    "        Losses[\"crossENT\"] = crossENT_dict\n",
    "        Losses[\"Huber\"] = Huber_dict\n",
    "        Losses[\"hinge\"] = hinge_dict\n",
    "\n",
    "        self.Loss_choices = Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9354054-fc41-4ad5-aeb3-ebd8d8b07511",
   "metadata": {
    "id": "a9354054-fc41-4ad5-aeb3-ebd8d8b07511"
   },
   "outputs": [],
   "source": [
    "class DeepNetwork:\n",
    "    def __init__ (self, X, Y, W_LB, W_UB, HL_dims, p_activ, p_loss, activ_choice, fin_activ_choice, loss_choice, class_markers):\n",
    "\n",
    "        ACs = set([\"relu\", \"binStep\", \"sigmoid\", \"tanh\", \"arctan\", \"prelu\", \"elu\", \"softplus\", \"Parctan\"])\n",
    "        LCs = set([\"L1\", \"L2\", \"crossENT\", \"Huber\", \"hinge\"])\n",
    "\n",
    "        if (activ_choice not in ACs) or (fin_activ_choice not in ACs) or (loss_choice not in LCs):\n",
    "\n",
    "            raise ValueError(\"Please enter a valid activator string: relu, binStep, sigmoid, tanh, arctan, prelu, elu, softplus, Parctan\")\n",
    "            raise ValueError(\"Please enter a valid loss string: L2, L1, crossENT, Huber, hinge\")\n",
    "            \n",
    "        else:\n",
    "            activ_dict = Activ(p_activ)\n",
    "            self.activator = activ_dict.Activ_choices[activ_choice]\n",
    "            self.fin_activator = activ_dict.Activ_choices[fin_activ_choice]\n",
    "\n",
    "            loss_dict = LossFuncs(p_loss)\n",
    "            self.losser = loss_dict.Loss_choices[loss_choice]\n",
    "\n",
    "            HL_dims.insert(0, X.shape[0])\n",
    "            HL_dims.append(Y.shape[0])\n",
    "            self.n = len(HL_dims)\n",
    "\n",
    "            self.weights_array = [None]*self.n\n",
    "            self.weights_array[0] = -1.0\n",
    "\n",
    "            self.bias_array = [None]*self.n\n",
    "            self.bias_array[0] = -1.0\n",
    "\n",
    "            for i in range (1,self.n):\n",
    "                self.weights_array[i] = np.random.uniform(W_LB, W_UB, (HL_dims[i],HL_dims[i-1]))\n",
    "                self.bias_array[i] = np.zeros((HL_dims[i],1))\n",
    "\n",
    "            self.Z_array = [None]*self.n\n",
    "            self.Z_array[0] = -1.0\n",
    "\n",
    "            self.Activ_array = [None]*self.n\n",
    "            self.Activ_array [0] = X\n",
    "\n",
    "            self.dW_array = [None]*self.n\n",
    "            self.dW_array[0] = -1.0\n",
    "\n",
    "            self.dZ_array = [None]*self.n\n",
    "            self.dZ_array[0] = -1.0\n",
    "\n",
    "            self.dA_array = [None]*self.n\n",
    "            self.dA_array[0] = -1.0\n",
    "\n",
    "            self.dB_array = [None]*self.n\n",
    "            self.dB_array[0] = -1.0\n",
    "\n",
    "            self.m = X.shape[1]\n",
    "            self.class_markers = class_markers\n",
    "            \n",
    "\n",
    "    def compute_Z (self,A_prev, W, B):\n",
    "        return (np.dot(W,A_prev) + B)\n",
    "\n",
    "    def forward_propogation (self,X):\n",
    "        self.Activ_array [0] = X\n",
    "        for i in range (1, self.n - 1):\n",
    "            self.Z_array[i] = self.compute_Z (self.Activ_array[i-1], self.weights_array[i], self.bias_array[i])\n",
    "            self.Activ_array[i] = self.activator[\"activ\"](self.Z_array[i])\n",
    "\n",
    "        self.Z_array[self.n - 1] = self.compute_Z(self.Activ_array[self.n - 2], self.weights_array[self.n - 1], self.bias_array[self.n - 1])\n",
    "        self.Activ_array[self.n - 1] = self.fin_activator[\"activ\"](self.Z_array[self.n - 1])\n",
    "\n",
    "\n",
    "\n",
    "    def computeGRADS (self,A_prev, W, B, dZ, m) :\n",
    "        dW = np.dot(dZ, A_prev.T)/m\n",
    "        dB = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        grads = {}\n",
    "        grads[\"dW\"] = dW\n",
    "        grads[\"dB\"] = dB\n",
    "        grads[\"dA_prev\"] = dA_prev\n",
    "        return grads\n",
    "\n",
    "    def backward_propogation (self,Y):\n",
    "        self.dA_array[self.n - 1] = self.losser[\"grad\"](self.Activ_array[self.n - 1],Y)\n",
    "        self.dZ_array[self.n - 1] = self.fin_activator[\"grad\"](self.Z_array[self.n - 1]) * self.dA_array[self.n - 1]\n",
    "        #in 1 run of the loop, we are given dZ[i], we compute: dA[i-1], dB[i], dW[i].\n",
    "        #then using dA[i-1] we also compute dZ[i-1], fully prepared for the next run of the loop.\n",
    "        for i in reversed(range(1,self.n)):\n",
    "            temp_grads = self.computeGRADS (self.Activ_array[i-1], self.weights_array[i], self.bias_array[i], self.dZ_array[i], self.m)\n",
    "            self.dW_array[i] = temp_grads[\"dW\"]\n",
    "            self.dB_array[i] = temp_grads[\"dB\"]\n",
    "            if (i-1 == 0):\n",
    "                break\n",
    "            else:\n",
    "                self.dA_array[i-1] = temp_grads[\"dA_prev\"]\n",
    "                self.dZ_array[i-1] = self.activator[\"grad\"](self.Z_array[i-1]) * self.dA_array[i-1]\n",
    "\n",
    "    def train_network (self,X,Y, num_iter, lrn_rate):\n",
    "        for i in range (0, num_iter):\n",
    "            self.forward_propogation(X)\n",
    "            self.backward_propogation(Y)\n",
    "            for j in range (1, self.n):\n",
    "                self.weights_array[j] -= lrn_rate * self.dW_array[j]\n",
    "                self.bias_array[j] -= lrn_rate * self.dB_array[j]\n",
    "                \n",
    "    \n",
    "    \n",
    "    def classif_predict (self, X_V):\n",
    "        self.forward_propogation(X_V)\n",
    "        array = self.Activ_array[self.n-1]\n",
    "        cmark = np.array(self.class_markers, dtype=int)\n",
    "        diffs = np.diff(cmark)\n",
    "        cumsum_half_diffs = np.cumsum(diffs / 2)\n",
    "        bin_edges = cmark[0] + cumsum_half_diffs\n",
    "        classes = cmark[np.round(np.searchsorted(bin_edges, array)).astype(int)] \n",
    "        return classes\n",
    "\n",
    "    def reg_predict (self, X_V):\n",
    "        self.forward_propogation(X_V)\n",
    "        return self.Activ_array[self.n - 1]\n",
    "\n",
    "    def get_train_methods(self):\n",
    "        print (\"__init__ (self, X, Y, W_LB, W_UB, HL_dims, p_activ, p_loss, activ_choice, fin_activ_choice, loss_choice)\")\n",
    "        print(\"train_network (self,X,Y, num_iter, lrn_rate)\")\n",
    "        print(\"lassif_predict (self, X_V, class_markers\")\n",
    "        print(\"reg_predict (self, X_V)\")\n",
    "\n",
    "    def get_parameters (self): \n",
    "        return self.weights_array, self.bias_array, HL_dims, p_activ, p_loss, \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c95da3-ddda-451a-9485-580b3d859c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 189)\n",
      "(30, 189)\n",
      "(30, 189)\n",
      "(1, 189)\n",
      "(1, 189)\n",
      "(1, 189)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "data = cancer.data\n",
    "target_ = cancer.target\n",
    "target = target_.reshape(target_.shape[0],1)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X,Y = shuffle(data, target) \n",
    "\n",
    "X_prev = X.T\n",
    "Y_prev = Y.T\n",
    "m = int(X_prev.shape[1]/3)\n",
    "\n",
    "def split_data(X_prev, Y_prev, m):\n",
    "\n",
    "    num_cols = X_prev.shape[1]  # Get the total number of columns\n",
    "    if num_cols < 3 * m:\n",
    "        raise ValueError(\"Number of columns must be at least 3 times the split size.\")\n",
    "\n",
    "    # Calculate the starting indices for each split\n",
    "    start_train = 0\n",
    "    start_val = start_train + m\n",
    "    start_test = start_val + m\n",
    "\n",
    "    # Split the matrices\n",
    "    X_train = X_prev[:, start_train:start_val]\n",
    "    X_val = X_prev[:, start_val:start_test]\n",
    "    X_test = X_prev[:, start_test:]\n",
    "    Y_train = Y_prev[:, start_train:start_val]\n",
    "    Y_val = Y_prev[:, start_val:start_test]\n",
    "    Y_test = Y_prev[:, start_test:]\n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
    "\n",
    "X_train, X_val, X_test_P, Y_train, Y_val, Y_test_P = split_data(X_prev, Y_prev, m)\n",
    "\n",
    "X_test = X_test_P[:, 0:189]\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "Y_test = Y_test_P[:, 0:189]\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "520ccf96-4e07-495d-a478-da831b7af7f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m W_LB \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m      6\u001b[0m W_UB \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m----> 8\u001b[0m cancer_classifier \u001b[38;5;241m=\u001b[39m DeepNetwork(X_train, Y_train, W_LB, W_UB, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, HL_dims, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossENT\u001b[39m\u001b[38;5;124m'\u001b[39m, class_markers)\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mDeepNetwork.__init__\u001b[1;34m(self, X, Y, W_LB, W_UB, HL_dims, p_activ, p_loss, activ_choice, fin_activ_choice, loss_choice, class_markers)\u001b[0m\n\u001b[0;32m     17\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m LossFuncs(p_loss)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosser \u001b[38;5;241m=\u001b[39m loss_dict\u001b[38;5;241m.\u001b[39mLoss_choices[loss_choice]\n\u001b[1;32m---> 20\u001b[0m HL_dims\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     21\u001b[0m HL_dims\u001b[38;5;241m.\u001b[39mappend(Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(HL_dims)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "# def __init__ (self, X, Y, W_LB, W_UB, HL_dims, p_activ, p_loss, activ_choice, fin_activ_choice, loss_choice, class_markers):\n",
    "class_markers = [0, 1]\n",
    "t = X_train.shape[0]\n",
    "HL_dims = [t, t, t-1, 2*t, t]\n",
    "W_LB = -0.5\n",
    "W_UB = 0.5\n",
    "\n",
    "cancer_classifier = DeepNetwork(X_train, Y_train, W_LB, W_UB, 0.1, 0.1, HL_dims, 'relu', 'sigmoid', 'crossENT', class_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a7c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9ee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee1492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
