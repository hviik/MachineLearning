{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70f81c63-ea37-41c4-b84e-985fb6323dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import load_digits\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfb32283-f2ba-4dd8-b05a-17cd79ca01bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer: \n",
    "    def __init__ (self, Id, Od, p_active, final = False):\n",
    "        self.W = (torch.randn(Od, Id) *  math.sqrt(2/Id)).to(device)\n",
    "        self.dW = torch.zeros_like(self.W).to(device)\n",
    "        \n",
    "        (self.B, self.dB, self.dB_rms, self.dB_vel, self.C, self.dC, self.dC_rms, \n",
    "        self.dC_vel, self.V_avg, self.C_avg, self.dV, self.dV_rms, self.dV_vel) = [torch.zeros(Od, 1).to(device) for _ in range (0,13)]\n",
    "        self.p = p_active\n",
    "        \n",
    "        self.V = torch.ones_like(self.C).to(device)\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.dA_prev = None\n",
    "        self.final = final \n",
    "        \n",
    "        def opt(self, opt_params):\n",
    "            f.optimize(self, opt_params)\n",
    "        \n",
    "        def fp (self, A_prev, Y):\n",
    "            if self.final:\n",
    "                f.fin_fprop(self, A_prev, Y)\n",
    "            else:\n",
    "                f.fprop(self, A_prev, Y)\n",
    "        \n",
    "        def bp (self, dA, Y): \n",
    "            if self.final:\n",
    "                f.fin_bprop(self, dA, Y)\n",
    "            else:\n",
    "                f.bprop(self, dA, Y)\n",
    "        \n",
    "        def test_fp(self, A_prev, Y):\n",
    "            fp(self, A_prev, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0609f83b-3d6e-4cac-b51b-e4583aa5bf5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class f: \n",
    "    @staticmethod\n",
    "    def prelu (Z,p): \n",
    "        return torch.where(Z>0, Z, p * Z)\n",
    "        \n",
    "    @staticmethod\n",
    "    def prelu_grad (Z,p): \n",
    "        return torch.where(Z>0, 1, p) \n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax (Z): \n",
    "        Z_exp = torch.exp(Z - torch.max(Z))\n",
    "        return Z_exp/torch.sum(Z_exp, dim = 0, keepdim = True)\n",
    "        \n",
    "    @staticmethod\n",
    "    def crossENT (A,Y): \n",
    "        m = A.shape[1]\n",
    "        L = -torch.sum(Y * torch.log(A), dim = 0, keepdim = True)\n",
    "        J = torch.sum(L)/m \n",
    "        return J\n",
    "    \n",
    "    @staticmethod\n",
    "    def fprop (layer, A_prev, Y):\n",
    "        e = 0.0000000000001\n",
    "        Z = (torch.matmul(layer.W, A_prev) + layer.B).to(device)\n",
    "        mean = torch.mean(Z, dim = 1, keepdim = True).to(device)\n",
    "        std = (torch.std(Z, dim = 1, keepdim = True) + e).to(device)\n",
    "        Z_ = ((Z - mean)/std).to(device)\n",
    "        H = ((layer.V * Z_) + layer.C).to(device)\n",
    "        A = (f.prelu(H, layer.p)).to(device)\n",
    "        layer.cache = {'A_prev': A_prev.to(device),\n",
    "                       'Z' : Z,\n",
    "                       'std': std,\n",
    "                       'H': H,\n",
    "                       'Z_': Z_,\n",
    "                       'A' : A\n",
    "                      }\n",
    "        \n",
    "    @staticmethod\n",
    "    def fin_fprop (layer, A_prev, Y):\n",
    "        e = 0.0000000000001\n",
    "        Z = (torch.matmul(layer.W, A_prev) + layer.B).to(device)\n",
    "        mean = torch.mean(Z, dim = 1, keepdim = True).to(device)\n",
    "        std = (torch.std(Z, dim = 1, keepdim = True) + e).to(device)\n",
    "        Z_ = ((Z - mean)/std).to(device)\n",
    "        H = ((layer.V * Z_) + layer.C).to(device)\n",
    "        A = (f.softmax(H, layer.p)).to(device)\n",
    "        layer.cache = {'A_prev': A_prev.to(device),\n",
    "                       'Z' : Z,\n",
    "                       'std': std,\n",
    "                       'H': H,\n",
    "                       'Z_': Z_,\n",
    "                       'A' : A\n",
    "                      }\n",
    "        self.cost = f.crossENT(A, Y)\n",
    "        \n",
    "    @staticmethod\n",
    "    def bprop (layers, dA, Y):\n",
    "            A_prev = layer.cache['A_prev'].to(device)\n",
    "            Z = layer.cache['Z'].to(device)\n",
    "            Z_ = layer.cache['Z_'].to(device)\n",
    "            A = layer.cache['A'].to(device)\n",
    "            m = A_prev.shape[1]\n",
    "            std = layer.cahse['std'].to(device)\n",
    "            dH = (dA * f.prelu_grad(H, layer.p)).to(device)\n",
    "            layer.dV = torch.sum((Z_ * dH), dim = 1, keepdim = True)\n",
    "            layer.dC = torch.sum(dH, dim = 1, keepdim = True)\n",
    "            dZ_ = (layer.V * dH).to(device)\n",
    "            dZ = (((m*dZ_) - (torch.sum(dZ_, dim = 1, keepdim = True)) - (Z_ * (toch.sum((Z_ * dZ_), dim = 1, keepdims = True))))/(m * std)).to(device)\n",
    "            layer.dW = torch.matmul(dZ, A_prev.permute(1, 0))\n",
    "            layer.dB = torch.sum(dZ, dim = 1, keepdim = True)\n",
    "            layer.dA_prev = torch.matmul(W.permute(1, 0), dZ)\n",
    "                             \n",
    "    @staticmethod\n",
    "    def fin_bprop (layers, dA, Y):\n",
    "            A_prev = layer.cache['A_prev'].to(device)\n",
    "            Z = layer.cache['Z'].to(device)\n",
    "            Z_ = layer.cache['Z_'].to(device)\n",
    "            A = layer.cache['A'].to(device)\n",
    "            m = A_prev.shape[1]\n",
    "            std = layer.cache['std'].to(device)\n",
    "            dH = ((A - Y)/m).to(device)\n",
    "            layer.dV = torch.sum((Z_ * dH), dim = 1, keepdim = True)\n",
    "            layer.dC = torch.sum(dH, dim = 1, keepdim = True)\n",
    "            dZ_ = (layer.V * dH).to(device)\n",
    "            dZ = (((m*dZ_) - (torch.sum(dZ_, dim = 1, keepdim = True)) - (Z_ * (toch.sum((Z_ * dZ_), dim = 1, keepdims = True))))/(m * std)).to(device)\n",
    "            layer.dW = torch.matmul(dZ, A_prev.permute(1, 0))\n",
    "            layer.dB = torch.sum(dZ, dim = 1, keepdim = True)\n",
    "            layer.dA_prev = torch.matmul(W.permute(1, 0), dZ)\n",
    "    \n",
    "    @staticmethod\n",
    "    def lr_decay(t, alpha_0):\n",
    "        alph = alpha_0\n",
    "        return alph\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimize(layers, opt_params):\n",
    "        alpha_0 = opt_params['alpha_0']\n",
    "        t = opt_params['t']\n",
    "        alpha = f.lr_decay(t, alpha_0)\n",
    "        beta_dW, beta_dW0 = opt_params['beta_dW'] \n",
    "        beta_dB, beta_dB0 = opt_params['beta_dB']\n",
    "        beta_dC, beta_dC0 = opt_params['beta_dC']\n",
    "        theta_C = opt_params['theta_C']\n",
    "        theta_V = opt_params['theta_V']\n",
    "        \n",
    "        layer.dB_vel = (((beta_dB) * (layer.dB_vel)) + ((1-Beta_dB) * (layer.dB)))/(1 - (beta_dB ** t))\n",
    "        layer.dB_rms = (((beta_dB0) * (layer.dB_vel)) + ((1-Beta_dB0) * ((layer.dB) ** 2)))/(1 - (beta_dB0 ** t))\n",
    "                                                        \n",
    "        layer.dW_vel = (((beta_dW) * (layer.dW_vel)) + ((1-Beta_dW) * (layer.dW)))/(1 - (beta_dW ** t))\n",
    "        layer.dW_rms = (((beta_dW0) * (layer.dW_vel)) + ((1-Beta_dW0) * ((layer.dW) ** 2)))/(1 - (beta_dW0 ** t))\n",
    "                                                        \n",
    "        layer.dV_vel = (((beta_dV) * (layer.dV_vel)) + ((1-Beta_dV) * (layer.dV)))/(1 - (beta_dV ** t))\n",
    "        layer.dV_rms = (((beta_dV0) * (layer.dV_vel)) + ((1-Beta_dV0) * ((layer.dV) ** 2)))/(1 - (beta_dV0 ** t))\n",
    "                                                        \n",
    "        layer.dC_vel = (((beta_dC) * (layer.dC_vel)) + ((1-Beta_dC) * (layer.dC)))/(1 - (beta_dC ** t))\n",
    "        layer.dC_rms = (((beta_dC0) * (layer.dC_vel)) + ((1-Beta_dC0) * ((layer.dC) ** 2)))/(1 - (beta_dC0 ** t))\n",
    "                                                        \n",
    "        layer.V_avg = (((theta_V) * (layer.V_avg)) + ((1 - theta_V) * (layer.V)))/(1 - (theta_V ** t))\n",
    "        layer.C_avg = (((theta_C) * (layer.C_avg)) + ((1 - theta_C) * (layer.C)))/(1 - (theta_C ** t))\n",
    "        \n",
    "        layer.W -= (alpha) * (layer.dW_vel/torch.sqrt(layer.dW_rms))\n",
    "        layer.B-= (alpha) * (layer.dB_vel/torch.sqrt(layer.dB_rms))\n",
    "        layer.V -= (alpha) * (layer.dV_vel/torch.sqrt(layer.dV_rms))\n",
    "        layer.C -= (alpha) * (layer.dC_vel/torch.sqrt(layer.dC_rms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bbcf4fd-6610-4642-9ffe-6ba87e18c56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TF:\n",
    "    @staticmethod\n",
    "    def fprop(layer, A_prev, Y):\n",
    "        e = 1e-8  # Small epsilon to avoid division by zero\n",
    "        device = A_prev.device\n",
    "        \n",
    "        # Forward pass\n",
    "        Z = torch.matmul(layer.W, A_prev) + layer.B  # Linear transformation\n",
    "        mean = torch.mean(Z, dim=1, keepdim=True)    # Mean across the first dimension (features)\n",
    "        std = torch.std(Z, dim=1, keepdim=True) + e  # Standard deviation (with epsilon)\n",
    "        Z_ = (Z - mean) / std                        # Normalization\n",
    "        \n",
    "        H = layer.V * Z_ + layer.C                   # Scale and shift\n",
    "        A = F.prelu(H, layer.p)                      # PReLU activation\n",
    "        \n",
    "        # Caching results for backpropagation\n",
    "        layer.cache = {'A_prev': A_prev, 'Z': Z, 'mean': mean, 'std': std, 'Z_': Z_, 'H': H, 'A': A}\n",
    "        \n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "768ea7a2-ee62-4a49-8808-e27cc4521cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model: \n",
    "    def __init__ (self, A0_dim, AL_dim, HL_dims, p_activ): \n",
    "        self.L_dims = [A0_dim] + HL_dims + [AL_dim]\n",
    "        self.n = len(self.L_dims)\n",
    "        self.layers = [0]\n",
    "        self.layers += [Layer(self.L_dims[i-1], self.L_dims[i], p_activ, final = False) for i in range (1, self.n - 1)]\n",
    "        self.layers += [Layer(self.L_dims[self.n - 2], self.L_dims[self.n - 1], p_activ, final = True)]\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        A_prev = X\n",
    "        for i in range(1, self.n): \n",
    "            \n",
    "            self.layers[i].fp(A_prev, Y)\n",
    "            A_prev = self.layers[i].cache['A']\n",
    "            #self.fp = F.fprop(self, A_prev, Y) \n",
    "    def backward(self, X, Y): \n",
    "        dA = None \n",
    "        for i in reversed(range(1, self.n)): \n",
    "            self.layers[i].bp(dA, Y)\n",
    "            dA = self.layers[i].dA_prev\n",
    "\n",
    "    def gradient_descent(self, alph): \n",
    "        for i in range(1, self.n): \n",
    "            self.layers[i].opt(opt_params)\n",
    "\n",
    "    def training_epoch (self, opt_params, X_train_batches, Y_train_batches, printCost = False):\n",
    "        bs = len(X_train_batches)\n",
    "        for t in range (0,bs): \n",
    "            X_train = X_train_batches[t]\n",
    "            Y_train = Y_train_batches[t]\n",
    "            \n",
    "            self.forward(X_train, Y_train)\n",
    "            self.backward(X_train, Y_train)\n",
    "            self.gradient_descent(alph)\n",
    "            if printCost:\n",
    "                print(self.layers[self.n-1].cost)\n",
    "        \n",
    "    def predict (self, X, Y): \n",
    "        self.forward(X, Y)\n",
    "        self.V = self.V_avg\n",
    "        fin_act = self.layers[self.n - 1].cache['A']\n",
    "        pred = torch.zeros_like(fin_act)\n",
    "        max_indices = torch.argmax(fin_act, dim=0)\n",
    "        pred.scatter_(0, max_indices.unsqueeze(0), 1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5e03060-8040-4918-a865-d987563ffa50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# If using CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "test_layer = Layer(3, 3, 0.3)\n",
    "test_layer.B = torch.tensor([0.1, 0.2, 0.3])\n",
    "test_layer.C = torch.tensor([0.02, 0.05, 0.04])\n",
    "test_layer.V = torch.tensor([0.22, 0.33, 0.212])\n",
    "A_prev = torch.tensor([[0.02, 0.04, 0.16], [0.04, 0.07, 0.11], [0.03, 0.03, 0.14]]).to(device)\n",
    "Y = torch.tensor([[0, 1, 0], [1, 0, 1], [0, 0, 1]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e66871a-55f6-497e-ba05-70a75b3ced8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          8.3369963   1.55676159  0.62348292  0.84159189  1.70138742\n",
      "  1.91442295 -0.12563545 -0.05955753  2.115924    0.92413236  0.90805457\n",
      "  0.50629541  0.40505773  0.82922685 -0.13061034 -0.04412544  0.89297863\n",
      "  0.9797157   0.834688   -1.15705571 -1.26139189 -0.54774412 -0.11488858\n",
      " -0.0333373  -0.47891083  0.71963665  0.82522503 -1.61198076 -1.28551251\n",
      " -0.63110674 -0.04722047  0.         -0.67669173  0.01640613  1.04693235\n",
      " -1.41038664 -1.49274895 -0.826222    0.         -0.06161515 -0.53487254\n",
      "  0.14031329  1.15830787 -1.22976866 -1.45846214 -0.79583077 -0.08996861\n",
      " -0.03590278  1.27829872  1.0757585   0.22534197 -1.80482941 -1.45713418\n",
      " -0.75553924 -0.21234332 -0.02359584  9.1623548   1.94719509 -1.42523668\n",
      " -2.37326603 -1.14176155 -0.50519614 -0.19619223]\n",
      "(64, 1500)\n",
      "(64, 200)\n",
      "(64, 97)\n",
      "(1797,)\n",
      "(1797, 10)\n",
      "0 -> [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(10, 1500)\n",
      "(10, 200)\n",
      "(10, 97)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Print dataset shape and target for the second sample\n",
    "print(digits.data.shape)\n",
    "print(digits.target[3])\n",
    "\n",
    "# Visualize the image of the second sample\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[3])\n",
    "plt.show()\n",
    "\n",
    "# Shuffle the data, keeping the corespondence between feature vector and it's label\n",
    "np.random.seed(1)\n",
    "data_, target_ = shuffle(digits.data, digits.target)\n",
    "\n",
    "#normalization\n",
    "\n",
    "X_prev = data_/np.linalg.norm(data_, axis = 1, keepdims = True)\n",
    "m = X_prev.shape[0]\n",
    "e = 0.00000001 #numerical stability\n",
    "X_prev_mean = np.sum(X_prev, axis = 0, keepdims = True)/m\n",
    "X_prev_deviation = np.sqrt(np.sum((X_prev - X_prev_mean)**2, axis = 0, keepdims = True)/m)\n",
    "X_ = (X_prev - X_prev_mean)/(X_prev_deviation + e)\n",
    "#check that normalization works\n",
    "print(X_[1])\n",
    "#we will take the first 1500 examples for training, the 200 examples for development, and the last 97 examples for test.\n",
    "#we will also transpose the matrix, so that our features are column vectors instead of row vectors.\n",
    "X_train = X_[:1500].T\n",
    "print(X_train.shape)\n",
    "X_dev = X_[1500:1700].T\n",
    "print(X_dev.shape)\n",
    "X_test = X_[1700:].T\n",
    "print(X_test.shape)\n",
    "#we see that now, the feature vectors are columns of our matrices, X_train, X_dev, X_test.\n",
    "\n",
    "Y_ = target_\n",
    "print(Y_.shape)\n",
    "num_classes = int(10) \n",
    "# we will initialize so that first we have 10 rows, then transpose, as usual. \n",
    "Y = np.zeros((Y_.shape[0], num_classes))\n",
    "print(Y.shape)\n",
    "#we will perform one hot encoding now. \n",
    "row_indices = np.arange(Y_.shape[0])\n",
    "Y[row_indices, Y_] = 1\n",
    "#it is clear that our one hot encoding works\n",
    "print(f\"{Y_[5]} -> {Y[5]}\")\n",
    "#now, we do the same train, test, dev, split on Y, and also transpose to ensure our ground truth vectors are columns\n",
    "Y_train = Y[:1500].T\n",
    "print(Y_train.shape)\n",
    "Y_dev = Y[1500:1700].T\n",
    "print(Y_dev.shape)\n",
    "Y_test = Y[1700:].T\n",
    "print(Y_test.shape)\n",
    "\n",
    "class Data: \n",
    "    def __init__ (self, X_train, X_dev, X_test, Y_train, Y_dev, Y_test): \n",
    "        self.XT_batches = []\n",
    "        self.YT_batches = []\n",
    "        batch_size = 300\n",
    "        for i in range(1, 6): \n",
    "            start_idx = (i - 1) * batch_size\n",
    "            end_idx = i * batch_size\n",
    "            self.XT_batches.append(X_train[:, start_idx:end_idx])\n",
    "            self.YT_batches.append(Y_train[:, start_idx:end_idx])\n",
    "            \n",
    "        self.X_dev = X_dev\n",
    "        self.Y_dev = Y_dev\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.A0_dims = X_train.shape[0]\n",
    "        self.AL_dims = Y_train.shape[0]\n",
    "\n",
    "data = Data(X_train, X_dev, X_test, Y_train, Y_dev, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8fdf986-f9a1-4b09-a856-c864bedd5543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming `data` is an object with attributes A0_dims and AL_dims\n",
    "q = int(data.A0_dims)\n",
    "A0 = int(data.A0_dims)\n",
    "AL = int(data.AL_dims)\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(6)\n",
    "n = 5\n",
    "\n",
    "# Initialize hidden layer dimensions (HL_dims) randomly\n",
    "HL_dims = torch.randint(q - 10, q + 10, (n,)).tolist()\n",
    "# Reset seed to 1 for consistency\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Assuming you have a Model class defined to create the model\n",
    "classif = Model(A0, AL, HL_dims, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "914ff4f9-276c-4681-9e1e-57967d6af839",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Layer' object has no attribute 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      3\u001b[0m     opt_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malph_0\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m : t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_dW\u001b[39m\u001b[38;5;124m'\u001b[39m : (\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), \n\u001b[0;32m      4\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_dB\u001b[39m\u001b[38;5;124m'\u001b[39m : (\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_dC\u001b[39m\u001b[38;5;124m'\u001b[39m : (\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), \n\u001b[0;32m      5\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_dV\u001b[39m\u001b[38;5;124m'\u001b[39m : (\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta_C\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta_V\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0.9\u001b[39m}\n\u001b[1;32m----> 7\u001b[0m     classif\u001b[38;5;241m.\u001b[39mtraining_epoch (opt_params, data\u001b[38;5;241m.\u001b[39mXT_batches, data\u001b[38;5;241m.\u001b[39mYT_batches, printCost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[42], line 32\u001b[0m, in \u001b[0;36mModel.training_epoch\u001b[1;34m(self, opt_params, X_train_batches, Y_train_batches, printCost)\u001b[0m\n\u001b[0;32m     29\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train_batches[t]\n\u001b[0;32m     30\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m Y_train_batches[t]\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_train, Y_train)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(X_train, Y_train)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_descent(alph)\n",
      "Cell \u001b[1;32mIn[42], line 13\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     10\u001b[0m A_prev \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn): \n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mfp(A_prev, Y)\n\u001b[0;32m     14\u001b[0m     A_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Layer' object has no attribute 'fp'"
     ]
    }
   ],
   "source": [
    "for t in range (0, 10):\n",
    "    \n",
    "    opt_params = {'alph_0' : 0.001, 't' : t, 'beta_dW' : (0.9, 0.999), \n",
    "               'beta_dB' : (0.9, 0.999), 'beta_dC' : (0.9, 0.999), \n",
    "               'beta_dV' : (0.9, 0.999), 'theta_C' : 0.9, 'theta_V' : 0.9}\n",
    "    \n",
    "    classif.training_epoch (opt_params, data.XT_batches, data.YT_batches, printCost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fcd0d-f796-43a3-ae94-2acccfbd4f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
