{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbea5ffe-f411-4dc9-87dc-6988aa9e6cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Print dataset shape and target for the second sample\n",
    "print(digits.data.shape)\n",
    "print(digits.target[3])\n",
    "\n",
    "# Visualize the image of the second sample\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[3])\n",
    "plt.show()\n",
    "\n",
    "# Shuffle the data, keeping the corespondence between feature vector and it's label\n",
    "np.random.seed(1)\n",
    "data_, target_ = shuffle(digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf34820-1b5b-4eb9-ac18-e4a5d4c4f0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          8.3369963   1.55676159  0.62348292  0.84159189  1.70138742\n",
      "  1.91442295 -0.12563545 -0.05955753  2.115924    0.92413236  0.90805457\n",
      "  0.50629541  0.40505773  0.82922685 -0.13061034 -0.04412544  0.89297863\n",
      "  0.9797157   0.834688   -1.15705571 -1.26139189 -0.54774412 -0.11488858\n",
      " -0.0333373  -0.47891083  0.71963665  0.82522503 -1.61198076 -1.28551251\n",
      " -0.63110674 -0.04722047  0.         -0.67669173  0.01640613  1.04693235\n",
      " -1.41038664 -1.49274895 -0.826222    0.         -0.06161515 -0.53487254\n",
      "  0.14031329  1.15830787 -1.22976866 -1.45846214 -0.79583077 -0.08996861\n",
      " -0.03590278  1.27829872  1.0757585   0.22534197 -1.80482941 -1.45713418\n",
      " -0.75553924 -0.21234332 -0.02359584  9.1623548   1.94719509 -1.42523668\n",
      " -2.37326603 -1.14176155 -0.50519614 -0.19619223]\n",
      "(64, 1500)\n",
      "(64, 200)\n",
      "(64, 97)\n"
     ]
    }
   ],
   "source": [
    "#normalization\n",
    "\n",
    "X_prev = data_/np.linalg.norm(data_, axis = 1, keepdims = True)\n",
    "m = X_prev.shape[0]\n",
    "e = 0.00000001 #numerical stability\n",
    "X_prev_mean = np.sum(X_prev, axis = 0, keepdims = True)/m\n",
    "X_prev_deviation = np.sqrt(np.sum((X_prev - X_prev_mean)**2, axis = 0, keepdims = True)/m)\n",
    "X_ = (X_prev - X_prev_mean)/(X_prev_deviation + e)\n",
    "#check that normalization works\n",
    "print(X_[1])\n",
    "#we will take the first 1500 examples for training, the 200 examples for development, and the last 97 examples for test.\n",
    "#we will also transpose the matrix, so that our features are column vectors instead of row vectors.\n",
    "X_train = X_[:1500].T\n",
    "print(X_train.shape)\n",
    "X_dev = X_[1500:1700].T\n",
    "print(X_dev.shape)\n",
    "X_test = X_[1700:].T\n",
    "print(X_test.shape)\n",
    "#we see that now, the feature vectors are columns of our matrices, X_train, X_dev, X_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f595925-ee0a-42d7-b702-d019a4d6441e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n",
      "(1797, 10)\n",
      "0 -> [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(10, 1500)\n",
      "(10, 200)\n",
      "(10, 97)\n"
     ]
    }
   ],
   "source": [
    "Y_ = target_\n",
    "print(Y_.shape)\n",
    "num_classes = int(10) \n",
    "# we will initialize so that first we have 10 rows, then transpose, as usual. \n",
    "Y = np.zeros((Y_.shape[0], num_classes))\n",
    "print(Y.shape)\n",
    "#we will perform one hot encoding now. \n",
    "row_indices = np.arange(Y_.shape[0])\n",
    "Y[row_indices, Y_] = 1\n",
    "#it is clear that our one hot encoding works\n",
    "print(f\"{Y_[5]} -> {Y[5]}\")\n",
    "#now, we do the same train, test, dev, split on Y, and also transpose to ensure our ground truth vectors are columns\n",
    "Y_train = Y[:1500].T\n",
    "print(Y_train.shape)\n",
    "Y_dev = Y[1500:1700].T\n",
    "print(Y_dev.shape)\n",
    "Y_test = Y[1700:].T\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b5e5c49-77ae-4da8-ab80-aeb086b5e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data: \n",
    "    def __init__ (self, X_train, X_dev, X_test, Y_train, Y_dev, Y_test): \n",
    "        self.XT_batches = []\n",
    "        self.YT_batches = []\n",
    "        batch_size = 300\n",
    "        for i in range(1, 6): \n",
    "            start_idx = (i - 1) * batch_size\n",
    "            end_idx = i * batch_size\n",
    "            self.XT_batches.append(X_train[:, start_idx:end_idx])\n",
    "            self.YT_batches.append(Y_train[:, start_idx:end_idx])\n",
    "            \n",
    "        self.X_dev = X_dev\n",
    "        self.Y_dev = Y_dev\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.A0_dims = X_train.shape[0]\n",
    "        self.AL_dims = Y_train.shape[0]\n",
    "\n",
    "data = Data(X_train, X_dev, X_test, Y_train, Y_dev, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b78454-f5cc-4a3a-8f31-19ea07eb4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F: \n",
    "    @staticmethod\n",
    "    def prelu (Z,p): \n",
    "        return np.where(Z>0, Z, p * Z)\n",
    "        \n",
    "    @staticmethod\n",
    "    def prelu_grad (Z,p): \n",
    "        return np.where(Z>0, 1, p) \n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax (Z): \n",
    "        Z_exp = np.exp(Z - np.max(Z))\n",
    "        return Z_exp/np.sum(Z_exp, axis = 0, keepdims = True)\n",
    "        \n",
    "    @staticmethod\n",
    "    def crossENT (A,Y): \n",
    "        m = A.shape[1]\n",
    "        L = -np.sum(Y * np.log(A), axis = 0, keepdims = True)\n",
    "        J = np.sum(L)/m \n",
    "        return J \n",
    "        \n",
    "    @staticmethod\n",
    "    def fprop (layer, A_prev, Y):\n",
    "        Z = np.dot(layer.W, A_prev) + layer.B\n",
    "        A = F.prelu (Z, layer.p)\n",
    "        layer.cache = {'A_prev': A_prev, 'Z' : Z, 'A' : A} \n",
    "\n",
    "    @staticmethod\n",
    "    def bprop (layer, dA, Y): \n",
    "        Z = layer.cache['Z']\n",
    "        A = layer.cache['A']\n",
    "        A_prev = layer.cache['A_prev']\n",
    "        \n",
    "        dZ = F.prelu_grad(Z, layer.p) * dA\n",
    "        layer.dW = np.dot(dZ, A_prev.T)\n",
    "        layer.dB = np.sum(dZ, axis = 1, keepdims = True)\n",
    "        layer.dA_prev = np.dot(layer.W.T, dZ)\n",
    "\n",
    "    \n",
    "    @staticmethod                      \n",
    "    def fin_fprop(layer, A_prev, Y): \n",
    "        Z = np.dot(layer.W, A_prev) + layer.B\n",
    "        A = F.softmax(Z)\n",
    "        layer.cache = {'A_prev': A_prev, 'Z' : Z, 'A' : A} \n",
    "        layer.cost = F.crossENT(A,Y)\n",
    "\n",
    "    \n",
    "    @staticmethod    \n",
    "    def fin_bprop (layer, dA, Y): \n",
    "        Z = layer.cache['Z']\n",
    "        A = layer.cache['A']\n",
    "        A_prev = layer.cache['A_prev']\n",
    "        \n",
    "        dZ = (A - Y)/m\n",
    "        layer.dW = np.dot(dZ, A_prev.T)\n",
    "        layer.dB = np.sum(dZ, axis = 1, keepdims = True)\n",
    "        layer.dA_prev = np.dot(layer.W.T, dZ)\n",
    "        \n",
    "    @staticmethod\n",
    "    def optimize (layer, alph): \n",
    "        layer.W -= alph * layer.dW\n",
    "        layer.B -= alph + layer.dB \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ecc8e64-3005-4678-b6fe-6681a3c3d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer: \n",
    "    def __init__ (self, Id, Od, p_activ, final = False):\n",
    "        self.p = p_activ\n",
    "        self.W = (np.random.randn(Od, Id)) * np.sqrt(2/(Id))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.B = np.zeros((Od, 1))\n",
    "        self.dB = np.zeros_like(self.B) \n",
    "        self.cache = {} \n",
    "        self.dA_prev = None\n",
    "        self.final = final\n",
    "        self.cost = None\n",
    "        \n",
    "    def opt(self, alph): \n",
    "        F.optimize(self, alph)\n",
    "        \n",
    "    def fp (self, A_prev, Y):\n",
    "        if self.final:\n",
    "            F.fin_fprop(self, A_prev, Y)\n",
    "        else:\n",
    "            F.fprop(self, A_prev, Y)\n",
    "            \n",
    "    def bp (self, dA, Y): \n",
    "        if self.final:\n",
    "            F.fin_bprop(self, dA, Y)\n",
    "        else:\n",
    "            F.bprop(self, dA, Y)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f982d311-4f8f-474c-b2c9-0dc8d5f3298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model: \n",
    "    def __init__ (self, A0_dim, AL_dim, HL_dims, p_activ): \n",
    "        self.L_dims = [A0_dim] + HL_dims + [AL_dim]\n",
    "        self.n = len(self.L_dims)\n",
    "        self.layers = [0]\n",
    "        self.layers += [Layer(self.L_dims[i-1], self.L_dims[i], p_activ, final = False) for i in range (1, self.n - 1)]\n",
    "        self.layers += [Layer(self.L_dims[self.n - 2], self.L_dims[self.n - 1], p_activ, final = True)]\n",
    "\n",
    "    def forward (self, X, Y):\n",
    "        A_prev = X\n",
    "        for i in range(1, self.n): \n",
    "            \n",
    "            self.layers[i].fp(A_prev, Y)\n",
    "            A_prev = self.layers[i].cache['A']\n",
    "            #self.fp = F.fprop(self, A_prev, Y) \n",
    "    def backward (self, X, Y): \n",
    "        dA = None \n",
    "        for i in reversed(range(1, self.n)): \n",
    "            self.layers[i].bp(dA, Y)\n",
    "            dA = self.layers[i].dA_prev\n",
    "\n",
    "    def gradient_descent(self, alph): \n",
    "        for i in range(1, self.n): \n",
    "            self.layers[i].opt(alph) \n",
    "\n",
    "    def training_epoch (self, alph, X_train_batches, Y_train_batches, printCost = False):\n",
    "        bs = len(X_train_batches)\n",
    "        for t in range (0,bs): \n",
    "            X_train = X_train_batches[t]\n",
    "            Y_train = Y_train_batches[t]\n",
    "            \n",
    "            self.forward(X_train, Y_train)\n",
    "            self.backward(X_train, Y_train)\n",
    "            self.gradient_descent(alph)\n",
    "            if printCost:\n",
    "                print(self.layers[self.n-1].cost)\n",
    "        \n",
    "    def predict (self, X, Y): \n",
    "        self.forward(X, Y)\n",
    "        fin_act = self.layers[self.n - 1].cache['A']\n",
    "        pred = np.zeros_like(fin_act)\n",
    "        max_indices = np.argmax(fin_act, axis=0)\n",
    "        pred[max_indices, np.arange(fin_act.shape[1])] = 1\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ff1b3e-5f38-44eb-8de3-b3662163f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = Data(X_train, X_dev, X_test, Y_train, Y_dev, Y_test)\n",
    "q = int(data.A0_dims)\n",
    "A0 = int(data.A0_dims)\n",
    "AL = int(data.AL_dims)\n",
    "np.random.seed(6)\n",
    "n = 5\n",
    "HL_dims = [np.random.randint(q-10, q + 10) for _ in range (0,n)]\n",
    "np.random.seed(1)\n",
    "classif = Model (A0, AL, HL_dims, 0.3)\n",
    "alph_0 = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c360887-9fe1-46f1-a2a0-061a5a0c838d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24063760994407773\n",
      "90.66666666666666\n"
     ]
    }
   ],
   "source": [
    "for _ in range (0,5000):\n",
    "    alph = alph_0\n",
    "    classif.training_epoch(alph, data.XT_batches, data.YT_batches)\n",
    "\n",
    "print(classif.layers[classif.n - 1].cost)\n",
    "train_pred = classif.predict(data.XT_batches[1], data.YT_batches[1])\n",
    "train_acc = 100*(1 - ((np.sum(np.abs(train_pred - data.YT_batches[1])))/data.YT_batches[1].shape[1]))\n",
    "print(train_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
